---
title             : "The role of cross-linguistic lexical similarity on bilingual word acquisition"
shorttitle        : "Cross-linguistic similarity and  word acquisition"

author: 
  - name          : "Gonzalo Garcia-Castro"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Ramon Trias Fargas, 25-27, 08005 Barcelona, Spain"
    email         : "gonzalo.garciadecastro@upf.edu"
  - name          : "Daniela Avila-Varela"
    affiliation   : "1"
  - name          : "Nuria Sebastian-Galles"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Center for Brain and Cognition, Universitat Pompeu Fabra, Barcelona, Spain"

authornote: |
  Campus de Ciutadella, Universitat Pompeu Fabra, 08005, Barcelona, Spain

abstract: |
  Bilinguals face the challenging task of learning words from languages with overlapping phonologies. Floccia et al. (2018) reported larger vocabulary sizes for 24-month-old bilinguals that were learning languages that shared a greater amount of cognates (e.g., English-Dutch). The mechanisms underlying this effect remain unknown. We explore two compatible scenarios. First, we test whether cognates are learnt earlier than non-cognates. This would account for the difference in vocabulary size associated to the amount of shared cognates across languages. Second, we explore the possibility that the word-forms of one language interact with those form the other language, scaffolding the acquisition of their translation equivalents when their phonologies overlap. This mechanism, in line with the parallel activation account of bilingual speech perception, would provide a plausible explanation to why cognates are acquired ealier by bilinguals. We developed an online tool to collect parental reports of receptive and productive vocabularies from children learning Catalan and/or Spanish, and present data on receptive and productive vocabulary of bilingual toddlers aged 12 to 34 months.


  <!-- https://tinyurl.com/ybremelq -->

keywords          : "lexical acquisition, vocabulary, bilingualism"
wordcount         : "X"

bibliography      : ["trajectories.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
keep_text         : false
class             : "man"

documentclass     : "apa6"
classoption       : "man"
output            :
  papaja::apa6_pdf:
    latex_engine  : "pdflatex"

  

---

```{r setup, include = FALSE}

knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE,
  cache.extra = knitr::rand_seed,
  out.width = "80%",
  results = "asis"
)
```

```{r packages, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

# load packages
library(papaja)
library(knitr)
library(here)
library(dplyr)
library(tidyr)
library(tidybayes)
library(scales)
library(janitor)
library(stringr)
library(ggplot2)
library(patchwork)

tar_load(multilex_data)
tar_load(items)
tar_load(participants)
tar_load(responses)
tar_load(loo_comp)
tar_load(loo_prod)
tar_load(fits_comp)
tar_load(fits_prod)

options(
  knitr.kable.NA = "-",
  knitr.duplicate.label = "allow",
  ggplot2.discrete.fill = wesanderson::wes_palettes$Darjeeling1,
  ggplot2.discrete.colour = wesanderson::wes_palettes$Darjeeling1,
  ggplot2.continuous.fill = ggplot2::scale_color_gradient,
  ggplot2.continuous.colour = ggplot2::scale_color_gradient
)

theme_set(theme_apa())

```

# Introduction

Before the end of their first year of life, infants start directing their gaze to some objects when hearing their labels, according to both experimental data [@jusczyk1995infants; @tincoff1999some; @bergelson2012months; @bergelson2015early] and parental reports [e.g., @fenson2007macarthur]. During the last half of their second year, they acquire new words at an increasingly fast rate [@goldfield1990early; @fenson1994variability; @mayor2011statistical; @bloom2002children; @bergelson2020comprehension]. These early stages of lexical acquisition are characterised by substantial variation between children, reflected, for instance, on the variation on the number of words they know [i.e. vocabulary size; @fenson1994variability; @frank2021variability] or on the proportion of those words that fall into the category of nouns, as opposed to verbs, adjectives, or function words [e.g., @nelson1973structure, bates1994developmental].


Despite this variability, children's trajectories of vocabulary growth seem quite stable across languages. @tardif2008baby collected data about the first ten words acquired by 10 to 16 month-old infants living in the United States, Hong Kong, and Beijing. Since birth, these infants had been learning English, Cantonese and Mandarin, respectively. The authors found a common pattern across the three groups: their first ten words referred to roughly the same concepts, namely relatives/caretakers (*daddy*, *mommy*), social routines (*bye*, *uh-oh*) or animals (*woof-woof*). These results were later extended by @frank2021variability to a wider diversity languages, who also reported that such cross-language commonalities are stronger at earlier stages of lexical acquisition, compared to later stages. Most of the literature on early word acquisition, however, has been conducted on monolingual children, and neglects the problem of how bilinguals--who represent a substantial proportion of the population in most societies--acquire words at early ages.


There is evidence that bilinguals know less words in each of their languages than monolinguals, but also that both groups know a similar amount of words when the two languages are aggregated. For example, @hoff2012dual found that English-Spanish bilingual toddlers in South Florida knew less words in English than monolinguals, who only learnt English. Both groups knew a similar total amount of words when both English and Spanish vocabularies were counted together, pointing to the importance of collecting data on both languages when assessing bilinguals' communicative development. Other studies have provided converging evidence that bilinguals know a similar--or even larger--number of words than monolinguals, only when the languages are aggregated [@oller2002language; @pearson1994patterns; @pearson1993lexical; @patterson2004comparing; @patterson2004bilingual; @smithson2014bilingualism; @petitto2001bilingual; @gonzalez2020bilingual]. While these studies have mostly relied on samples of bilingual children learning two relatively distant languages, as it is the case of English and Spanish, it is unclear whether children learning typologically more similar languages also know less words in each of their languages than monolinguals. What role could linguistic distance play during early vocabulary growth?


For a given set of concepts, bilingual children may be exposed to two distinct sets of word-forms--one in each language. Depending on the linguistic distance between both languages, the two sets of words may overlap in varying degrees. Particularly, when both languages are typologically close, like Spanish and Catalan (both Roman languages), they are more likely to share a large amount of cognates (i.e., form-similar translation equivalents) than two linguistically distant languages, like Spanish and English (one Roman, the other Germanic). For instance, in the presence of a door, a Spanish-Italian (or a Spanish-Catalan) bilingual might hear *puerta* and *porta* (cognates), whereas a Spanish-English bilingual might hear *puerta* and *door* (non-cognates). It could be the case that mapping two phonologically similar labels (cognates like *puerta*-*porta*) onto the same referent is easier than doing the same with two phonologically dissimilar labels (non-cognates, like *puerta* and *door*). If cognates are easier to acquire than non-cognates, bilinguals learning a pair of languages that share a high proportion of cognates should benefit more often from this facilitation effect than those learning a pair of languages with a lower proportion of cognates, and should therefore show larger vocabulary sizes.


@floccia2018introduction provided evidence in line with this claim. The authors collected vocabulary data on word comprehension and production from 372 24-month-old bilingual toddlers living in the United Kingdom who were learning English and an additional language. The additional language was one a pool of 13 typologically diverse languages: Bengali, Cantonese Chinese, Dutch, French, German, Greek, Hindi/Urdu, Italian, Mandarin Chinese, Polish, Portuguese, Spanish and Welsh. The authors calculated the average phonological similarity between the words in each of these additional languages and their translation equivalents in English. Phonological similarity was measured by computing the Levenshtein distance between each cross-language pair of phonological transcriptions. The Levenshtein distance is a metric that computes the edit distance between two strings by counting the smallest number of insertions, deletions and substitutions one of the strings has to go through to become identical to the other [@levenshtein1966binary]. The resulting scores were then divided by the length of the longest string to bound the similarity scores between 0 and 1, and then entered this variable as a predictor as they modelled participants' vocabulary sizes. Among other findings, the authors reported an increase in productive vocabulary size in the additional language associated with an increase in the average phonological similarity between the translation equivalents of each language pair. For example, English-Dutch bilinguals (22.14% phonological similarity), were able to produce more Dutch words than English-Mandarin bilinguals (1.97% phonological similarity) were able to produce in Mandarin.


Floccia et al. pointed to *parallel activation* as the main mechanism underpinning their results. The parallel activation hypothesis suggests that bilinguals activate both languages simultaneously during speech production or comprehension. This phenomenon is the result of the activation of lexical representations in both languages, even when only one is in use during production [@costa2000cognate; @hoshino2008cognate] or comprehension [@thierry2007brain]. One of the clearest examples of parallel activation was provided by @costa2000cognate. In this study, Catalan-Spanish monolingual and bilingual adults were asked to name pictures of common objects in Spanish. In half of the trials, the object labels were cognates in Spanish and Catalan (*árbol*-*arbre*, translations of *tree*), whereas in the other half of the trials labels were non-cognates (*mesa*-*taula*, translations of *table*). Bilinguals named cognate pictures faster than non-cognate pictures, even after adjusting for the lexical frequency of the items. Importantly, Spanish monolinguals did not show this effect. These results suggest that, for the bilinguals, Catalan phonology was activated during the production of Spanish words, facilitating the naming of cognate pictures. Several recent studies have also provided similar evidence in comprehension in children [e.g., @von2012language; @poulin2013lexical]. Parallel activation is therefore a plausible mechanism to account for Floccia et al.'s results: cognates increase the amount of activation in both languages, facilitating word acquisition, and ultimately leading to children learning language pairs with a larger proportion of cognates are predicted to show larger vocabulary sizes. A missing piece in this account is how increase parallel activation might facilitate the acquisition new words.


In this study, we hypothesise that cognates are acquired earlier in age than non-cognates, which might explain why children learning languages that share many cognates show larger vocabulary sizes. Evidence supporting this claim is scarce. To the best of our knowledge, only two studies have provided direct data on this issue. First, @bosch2014first used vocabulary parental reports (152 lexical items) from 48 Catalan-Spanish bilinguals aged 18 months, and found that cognates represented a larger proportion of participant's vocabulary than non-cognates. Second, @schelletter2002effect reported a longitudinal single case of one English-German bilingual who produced cognates earlier than non-cognates, on average. The low sample size in these studies makes it challenging to draw strong conclusions about the effect of cognateness on vocabulary growth. Floccia et al.'s estimates are statically more reliable given their (much larger) sample size, but their study was not aimed at testing the effect of cognateness on age of acquisition directly. In their discussion the authors state the following (pp. 70):


> "This  finding  also  provides  support  to  the  proposal  that  the  cognate advantage is due to cognates being acquired before non-cognates in early childhood (Costa et al., 2016), leading to an ease of processing later in life."


We identify two main reasons why an earlier age of acquisition for cognates than for non-cognates is an unwarranted conclusion from Floccia et al.'s results. First, the response variable used was the proportion of words each participant understood and/or produced (i.e., vocabulary size), from the list of lexical items in the vocabulary checklists. By aggregating the responses from all items into a single datum per child, information about the acquisition status of cognates vs. non-cognates was lost. Second, all participants were aged ~24 months, meaning that even if the unaggregated responses to individual items were included as response variable, the possible effect of cognateness could only be interpreted as an increase or decrease in the likelihood of participants at such age to have acquired each item, and not as an increase or decrease in the age of acquisition of such item. In summary, the current evidence supporting an earlier age of acquisition of cognates vs. non-cognates presents some limitations that prevents drawing sound conclusions about this effect. 


Our second hypothesis detaches from the fact that, for parallel activation to take place, and for cognateness to facilitate the acquisition of a word form in a given language, the child must be familiar with the (phonologically similar) translation of the word, in the other language. If the child is not previously familiar with one of the word forms, the acquisition of its translation in  language cannot be facilitated by cognateness. This implies that, if cognateness facilitates the acquisition of a word form, it can only do so after the child has acquired one of the word forms of the translation pair. Given that children are more likely to acquire words from languages to which they are exposed more often [@david2008individual; @cattani2014much; @thordardottir2006bilingual], the acquisition of words in the language of lower exposure should, on average, be more susceptible to the effect of cognateness. Therefore, we hypothesised that the effect of cognateness for words in the language of lower exposure would be larger than in the language of higher exposure.


In line with this hypothesis, the vocabulary size associated with linguistic similarity that Floccia et al. reported was larger in the additional language vocabulary than in English vocabulary. Most participants in their sample were English-dominant, meaning that their relative amount of exposure to English was larger than in the additional language. Therefore, participants may have, on average, learnt the English word-form of translation equivalents earlier than the word-form in the additional language.  If this is the case, then the acquisition of English words by English-dominant participants would rarely benefit from their cognate status (the other word-form is not available yet), while the acquisition of words in the additional language would benefit from their phonological similarity with the (available) English form. To test this hypothesis, we conditioned the effect of cognateness to the relative degree of exposure to the language.


In summary, we investigated the role of cognateness in bilingual word acquisition (conditional to the degree of relative exposure to the language) as possible mechanism to account for the facilitation  of language similarity on bilingual children's vocabulary size. We hypothesised that cognate words would be acquired earlier than non-cognate words, and that this effect should be larger for words in the language with lower exposure. Using an online vocabulary checklist--carefully designed specifically for this study--we collected data from a sample of children aged `r min(floor(participants$age))` to `r max(floor(participants$age))` months learning Catalan and/or Spanish, with varying degrees of exposure to each language. We then modelled the probability of participants being reported by their parents to understand and/or say each word in the checklist, conditional to its cognate status in Catalan and Spanish and participants' degree of exposure to the corresponding language, while adjusting for participants' age and the item's lexical frequency.


# Methods

Data and materials are available at OSF (https://osf.io/hy984/), and code is available at GitHub (https://github.com/gongcastro/trajectories).

## Participants

```{r participants, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}


n_participants <- length(unique(participants$id))
dates <- as_date(range(participants$time_stamp, na.rm = TRUE))

```

We collected data from `r printnum(n_participants)` children from the Metropolitan Area of Barcelona between `r format(dates[1], "%dth %B, %Y")` and `r format(dates[2], "%dth %B, %Y")`. All families gave informed consent before participating and this study was approved by the Comitè d'Ètica de la Investigació amb Medicaments (CEIm) from Hospital del Mar (Barcelona, Spain), code XXXXXXXXX. Families were contacted by e-mail or phone if their child were aged between 10 and 34 months, and had not previously reported to be exposed more than 10% of the time to a language other than Spanish or Catalan. Upon consent, families were sent a link to the questionnaire via e-mail, which they filled from a computer, laptop, or mobile device in a browser within the two week following the invitation to participate. Table 1 summarises the distribution of participants across ages and degrees of exposure (DoE) to the non-dominant languag (i.e., their degree of bilingualism).

```{r participants_lp, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Participant sample size by age and degree of exposure to a second language. Information about DoE was provided by families when filling the questionnaire. For illustration purposes this table, DoEs were binned into 10\\% wide bins, and ages (in months) were round to the nearest integer. Hyphens indicate that no participants from that specific combination of age filled the questionnaire."}
labs <- c("0-10%", "10-20%", "20-30%", "30-40%", "40-50%")
participants %>% 
  mutate(
    age = round(age),
    doe_2 = ifelse(dominant_language=="Catalan", doe_spanish, doe_catalan),
    doe_2 = cut(doe_2, breaks = seq(0, 0.5, 0.1), include.lowest = TRUE, labels = labs)
  ) %>% 
  count(age, doe_2) %>% 
  arrange(doe_2) %>% 
  mutate(doe_2 = fct_inorder(doe_2)) %>% 
  pivot_wider(names_from = age, values_from = n, values_fill = NA) %>% 
  gt() %>% 
  tab_spanner(label = "Number of participants by age (months)", columns = 2:17) %>% 
  fmt_missing(everything(), everything(), missing_text = "-") %>% 
  cols_label(
    doe_2 = md("**L2 DoE**")
  ) %>% 
  tab_style(
    cell_text(style = "italic"),
    cells_column_labels(everything())
  ) %>% 
  tab_style(
    cell_text(weight = "bold"),
    cells_column_spanners(everything())
  )

```


We use the highest educational attainment of parents or caretakers as a proxy of participants' socio-economic status (SES), which families self-reported in the questionnaire by filling two items asking for the educational attainment of each parent or caretaker, with the following available options: *No education*, *Primary*, *Secondary*, *Complementary*, *Vocational*, and *University*, in line with the current educational system in Spain. Most families reported university studies (`r table(participants$edu_parent)["University"]`, `r table(participants$edu_parent)["University"]/length(unique(participants$id))`), followed by families were the highest educational attainment were vocational studies (`r table(participants$edu_parent)["Vocational"]`, `r percent(table(participants$edu_parent)["Vocational"]/length(unique(participants$id)))`), complementary studies (`r table(participants$edu_parent)["Complementary"]`,  `r percent(table(participants$edu_parent)["Complementary"]/length(unique(participants$id)))`), secondary education (`r table(participants$edu_parent)["Secondary"]`, `r percent(table(participants$edu_parent)["Secondary"]/length(unique(participants$id)))`, `r percent(table(participants$edu_parent)["Primary"]/length(unique(participants$id)))`), no formal education (`r table(participants$edu_parent)["No education"]`, `r percent(table(participants$edu_parent)["No education"]/length(unique(participants$id)))` and primary education (`r table(participants$edu_parent)["Primary"]`, <1%). Highest educational attainment was similar across participants with different DoE to a second language, and therefore, differences in comprehension or production between participants with different language profiles are unlikely to stem from a difference in SES [@fernald2013ses] (see Table 2). 

```{r participants_edu, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Educational attainment. Proportion of highest parental educational attainment by DoE to the second language. For illustration purposes, DoEs were binned into 10\\% wide bins."}
labs <- c("0-10%", "10-20%", "20-30%", "30-40%", "40-50%")

participants %>%
  mutate(
    age = round(age),
    doe_2 = ifelse(dominant_language=="Catalan", doe_spanish, doe_catalan),
    doe_2 = cut(doe_2, breaks = seq(0, 0.5, 0.1), include.lowest = TRUE, labels = labs)
  ) %>% 
  group_by(doe_2) %>%
  summarise(
    n_no_education = length(edu_parent[edu_parent=="No education"])/n(),
    n_primary = length(edu_parent[edu_parent=="Primary"])/n(),
    n_secondary = length(edu_parent[edu_parent=="Secondary"])/n(),
    n_complementary = length(edu_parent[edu_parent=="Complementary"])/n(),
    n_vocational = length(edu_parent[edu_parent=="Vocational"])/n(),
    n_university = length(edu_parent[edu_parent=="University"])/n(),
    .groups = "drop"
  ) %>% 
  gt() %>% 
  tab_spanner("Educational attainment", columns = 2:7) %>% 
  cols_label(
    doe_2 = md("**L2 DoE**"),
    n_no_education = "No education",
    n_primary = "Primary",
    n_secondary = "Secondary",
    n_complementary = "Complementary",
    n_vocational = "Vocational",
    n_university = "University"
  ) %>% 
  fmt_percent(2:7) %>% 
  fmt_missing(everything(), missing_text = "-") %>% 
  tab_style(
    cell_text(style = "italic"),
    cells_column_labels(everything())
  ) %>% 
  tab_style(
    cell_text(weight = "bold"),
    cells_column_spanners(everything())
  )
```

## Questionnaire

```{r items, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
pool <- multilex::pool %>% 
  drop_na(cognate)
n_categories <- distinct(pool, category) %>% nrow()
n_items <- length(unique(pool$item))
n_item_language <- count(pool, language) %>% pull(n) %>% set_names(c("catalan", "spanish"))
n_item_cognate <- count(pool, language, cognate) %>% 
  pull(n) %>% 
  set_names(c("catalan-cognate", "catalan-noncognate", "spanish-cognate", "spanish-noncognate"))
```


The questionnaire was implemented on-line using the formR platform [@arslan2020formr], and was structured in three blocks: a (1) language questionnaire, a (2) demographic survey, and a (3) Catalan and a Spanish vocabulary checklists. Vocabulary checklists followed a similar structure as the Oxford Communicative Developmental Inventory [@hamilton2000infant] and consisted in two lists of words: one in Catalan and one in Spanish. The Catalan inventory contained `r n_item_language["catalan"]` items (`r n_item_cognate["catalan-cognate"]` cognates, `r n_item_cognate["catalan-noncognate"]` non-cognates) and the Spanish inventory contained `r n_item_language["spanish"]` (`r n_item_cognate["spanish-cognate"]` cognates, `r n_item_cognate["spanish-noncognate"]` non-cognates). Items in one language were translation equivalents of the items in the other language (e.g., whenever *gos* [dog] was included in the Catalan inventory, the word *perro* was included in the Spanish inventory), roughly following a one-to-one mapping. When there were two acceptable translation equivalents for a given word, we included both in separate items (e.g., Catalan *acabar* [*to finish*] and Spanish *acabar* and *terminar*), or merged them into a single items (e.g., Spanish *mono* [*monkey*] and Catalan *mono/mico*. We included items from a diverse sample of `r n_categories` semantic/functional categories (see Appendix 1). For the analyses included in this study, we excluded items from the adverbs, auxiliary words, connectives, interjections and games and routines categories, so that only data from content words (nouns, adjectives, and verbs) were used.


For each word in the vocabulary checklists, we asked parents to report whether their child was able to understand it, understand *and* say it, or did not understand or say it (checked out by default). Some families filled a long version of the vocabulary checklists (800 translation equivalents; 800 items in Catalan, 800 items in Spanish), while others filled a shorter version (~400 translation equivalents, ~400 items in Catalan, ~400 items in Spanish). These last families were randomly allocated into one of four different subsets of the complete list of items. These lists were carefully designed so that each contained a representative subsample of the items from the complete list. Semantic/functional categories with less than 16 items--thus resulting in less than four items after dividing it in four lists--were not divided in the short version of the questionnaire: all of their items were included in the four lists. Another subset of items that were part of the trial lists of some experiments in the lab were also included in all versions. Table 2 in Appendix 1 shows the distribution of items across questionnaire versions. We excluded from the analysis multi-word items (e.g., *barrita de cereales* [cereal bar]) and items that included more than one word-form (e.g., *mono / mico*). Table 3 shows the classification of items in cognates and non-cognates and their lexical frequency scores across the four lists of the inventories.


```{r items_frequency, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Lexical frequencies. Mean, standard error, and 95\\% confidence interval of lexical frequencies of items included in the Catalan and Spanish lists, reported separately for identical cognates, non-identical cognates, and non-cognates."}
items %>%
  select(te, language, cognate, frequency) %>% 
  left_join(select(multilex_data$pool, te, language, version)) %>% 
  unnest(version) %>% 
  group_by(language, version, cognate) %>% 
  summarise(
    frequency_mean = mean(frequency, na.rm = TRUE),
    frequency_sd = sd(frequency, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) %>% 
  mutate(
    frequency_se = frequency_sd/sqrt(n),
    frequency_ci_lower = frequency_mean + qnorm(0.025)*frequency_se,
    frequency_ci_upper = frequency_mean + qnorm(0.975)*frequency_se
  ) %>% 
  pivot_wider(names_from = c("cognate"), values_from = c("n", matches("frequency"))) %>% 
  clean_names() %>% 
  mutate(
    frequency_ci_identical_cognate = paste0(
      "[", round(frequency_ci_lower_identical_cognate, 2), ", ", round(frequency_ci_lower_identical_cognate, 2), "]"
    ),
    frequency_ci_non_identical_cognate = paste0(
      "[", round(frequency_ci_lower_non_identical_cognate, 2), ", ", round(frequency_ci_lower_non_identical_cognate, 2), "]"
    ),
    frequency_ci_non_cognate = paste0(
      "[", round(frequency_ci_lower_non_cognate, 2), ", ", round(frequency_ci_lower_non_cognate, 2), "]"
    ),
  ) %>% 
  relocate(
    language, version,
    n_identical_cognate,
    frequency_mean_identical_cognate,
    frequency_se_identical_cognate,
    frequency_ci_identical_cognate,
    matches("_non_identical"),
    matches("_non_cognate")
  ) %>% 
  mutate(language = ifelse(language=="Catalan", "CAT", "SPA")) %>% 
  select(-matches("sd|upper|lower")) %>% 
  gt(groupname_col = "language") %>% 
  fmt_number(matches("mean|se")) %>% 
  tab_spanner("Identical cognates", columns = ends_with("_identical_cognate")) %>% 
  tab_spanner("Non-identical cognates", columns = ends_with("_non_identical_cognate")) %>% 
  tab_spanner("Non-cognates", columns = ends_with("_non_cognate")) %>% 
  cols_label(
    version = "List",
    n_identical_cognate = md("N"),
    n_non_identical_cognate = md("N"),
    n_non_cognate = md("N"),
    frequency_mean_identical_cognate = md("M"),
    frequency_mean_non_identical_cognate = md("M"),
    frequency_mean_non_cognate = md("M"),
    frequency_se_identical_cognate = md("SD"),
    frequency_se_non_identical_cognate = md("SD"),
    frequency_se_non_cognate = md("SD"),
    frequency_ci_identical_cognate = md("95% *CI"),
    frequency_ci_non_identical_cognate = md("95% CI"),
    frequency_ci_non_cognate = md("95% CI")
  ) %>% 
  cols_width(
    starts_with("frequency_ci_") ~ px(100),
    TRUE ~ px(40)
  ) %>% 
  tab_style(
    cell_text(style = "italic"),
    cells_column_labels(everything())
  ) %>% 
  tab_style(
    cell_text(weight = "bold"),
    cells_column_spanners(everything())
  )


```

## Data analysis

```{r n_responses, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
n_obs <- nrow(responses)
sum_obs <- responses %>%
  group_by(te) %>%
  summarise(n = n(), .groups = "drop") %>%
  summarise(
    mean = mean(n),
    sd = sd(n),
    min = min(n),
    max = max(n)
  )

```


We gathered `r printnum(n_obs, big.mark = ",")` observations, with each observation corresponding to a single reponse of one participant to a single item. Translation equivalents (TEs) received an average of `r printnum(sum_obs$mean, digits = 2)` responses (*SD* = `r printnum(sum_obs$sd, digits = 2)`, *Min* = `r printnum(sum_obs$min, digits = 2)`, Max = `r printnum(sum_obs$max, digits = 2)`), both languages summed together. To investigate our hypotheses, we modelled the probability of children understanding or producing TEs using a logistic regression model with a Bernoulli distribution and a logit link function [@agresti2003categorical]. We included several predictors of interest as fixed effects to adjust this probability to participants' and items' characteristics, namely:


* *Age* of participant in months (`age`), calculated as the difference in days between participants' birth date and questionnaire completion divided by 30, chunked into bins of two months width, and centred around the mean.
* *Lexical frequency* of item (`frequency_center`), Relative lexical frequencies (frequency per million words, $fpmw$) were extracted from the CHILDES data base, using the `childesr` R package [@braginsky2018childesr], and were transformed to Zipf scores [@van2014subtlex; @zipf1949human] ($log10(fpmw)+3$). Missing frequencies were imputed by multiple imputation using the `mice` R package [van2011mice]. The resulting scores were then centred around the mean.
* *Degree of exposure* to the language (`doe_center`), computed as the proportion of the time the child is exposed to the language the item belongs to. For instance, for a response to the item `perro` (Spanish), the degree of exposure would correspond to the participant's relative exposure to Spanish (ranging from 0 to 1). Before entering the analyses, this variable was centred it around the mean and divided by 10, so that the values of the regression coefficients of this predictor correspond to a 10% increase in exposure to the language of the item.
* *Cognateness*, a categorical variable indicating whether the Catalan and Spanish forms of the TE are identical cognates (IC), non-identical cognates (NIC), or non-cognates (NC), as rated by a trained Spanish-Catalan bilingual linguist. We sum-coded two contrasts [@schad2020capitalize]: `cognate1`, comparing IC and NIC (both coded as +0.25) with NC (coded as -0.5) and `cognate2`, comparing IC (coded as +0.5) with NIC (coded as -0.5).


We first fit a base model that only included the main effects of `age` and `frequency_center`  (Model 1) [see @braginsky2019consistency; @jones2019children; @kachergis2021toward for similar approaches]. We then entered our predictors of interest by adding `doe_center` (Model 2) and then `cognate` and its interaction with `doe_center` (Model 3). To account for the hierarchical structure of the data (some responses to different TEs corresponded to the same participant, and some responses from different participants corresponded to the same TE )every model included *Participants* (`id`) and *TEs* (`te`) as grouping variables (i.e. random effects), with all possible random intercepts and slopes where appropriate [@barr2013random] (see Appendix for more details of the model). 


We adopted a Bayesian approach toward statistical inference. This approach allows to (1) incorporate previous domain knowledge into the inference process implementing the Bayes theorem, and (2) to quantify the uncertainty associated to the estimated parameters in our model [@mcelreath2018statistical]. 
We fit the models using the R package `brms` [@burkner2017brms], an R interface to the probabilistic language Stan [@carpenter2017stan]. We run four sampling chains with 1,000 iterations each, including 500 warm-up iterations per chain. All R-hat values were smaller than 1.1, indicating that the chains converged successfully in all cases [@gelman1992inference] (see Appendix for an extended description of model diagnostics).


We then compared comprehension and production models separately using leave-one-out cross-validation (LOO-CV) [@vehtari2017practical]^[Due to the high computational cost associated with our large dataset, we used a sub-sampling approach for performing Bayesian LOO with 1,000 samples [@magnusson2019bayesian]]. This method computes, for a given model, the sum of the log scores of the posterior predictive distributions that result from removing one data-point at a time, providing an estimate of the model fit. The LOO estimates are adjusted for the number of parameters estimated in the model, therefore accounting for overfitting. We compared the expected log posterior density (ELPD) of each each model to test which one fitted the data the best.


We then explored the posterior distribution of each parameter in the comprehension and production models that fitted the data the best computing the 95% credible intervals and testing whether this interval excluded 0. Credible intervals (CrI) indicate the range of values we are 95% certain contain the true value of the parameter, conditional to the observed data. We performed follow-up tests on interactions whose 95% credible interval excluded 0 by comparing the 95% CrI of their expected posterior marginal means using the `tidybayes` R package [@R-tidybayes]. Data processing and visualisation was done in R [@R-base] using the `tidyverse` family of packages [@R-tidyverse].


# Results

```{r coefs, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

coef_comp <- as.data.frame(fixef(fits_comp$fit_2))
coef_prod <- as.data.frame(fixef(fits_prod$fit_2))

```


For both comprehension and production, the extended model--which included all main effects and interactions--fitted data the best: it showed the smallest absolute expected log predictive density. This estimate is several times larger than its standard error indicating high predictive accuracy. Table 3 summarises the LOO scores of the fitted models. In the next sections we report the fixed effects of the comprehension and production models separately. We report the mean of each coefficient's posterior distribution along with standard errors and 95% CrIs in Tables 4 and 5. For interpretability, we report the derivative of the logistic function for each coefficient^[The derivative is calculated as $\hat{\beta_j}/4$, where $\hat{\beta_j}$ is the estimated mean of the posterior distribution of coefficient $j$. This value corresponds to the slope of the logistic curve at the midpoint, where it is steepest.] for each coefficient, which indicates the maximum difference in probability corresponding to one unit different in the input variable [@gelman2020regression]. 

```{r loo_prod, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

l_comp <- loo_comp %>% 
  as_tibble() %>% 
  rownames_to_column("model") %>% 
  select(-matches("p_loo|looic")) %>% 
  relocate(model, matches("elpd_loo"), matches("diff")) %>%
  mutate_all(as.numeric) %>% 
  mutate(type = "Comprehension", model = paste0("Model ", model))

l_prod <- loo_prod %>% 
  as_tibble() %>% 
  rownames_to_column("model") %>% 
  select(-matches("p_loo|looic")) %>% 
  relocate(model, matches("elpd_loo"), matches("diff")) %>%
  mutate_all(as.numeric) %>% 
  mutate(type = "Production", model = paste0("Model ", model))


l <- bind_rows(l_comp, l_prod) %>% 
  mutate_all(function(x) ifelse(x==0, NA, x))

gt(l, groupname_col = "type") %>% 
  tab_spanner(md("**ELPD</sub>**"), matches("elpd_loo")) %>% 
  tab_spanner(md("**Diff.**"), matches("diff")) %>%
  fmt_number(2:7) %>% 
  fmt_missing(columns = everything(), missing_text = "-") %>% 
  cols_label(
    model = "Model",
    # elpd
    elpd_loo = md("*ELPD*"),
    se_elpd_loo = md("*SE*"),
    subsampling_se_elpd_loo = md("*SE* (sub)"),
    # diff
    elpd_diff = md("*diff*"),
    se_diff = md("*SE*"),
    subsampling_se_diff = md("*SE* (sub)"),
  )
```


# Comprehension

The average probability of participants understanding a TE was `r printnum(inv_logit_scaled(coef_comp["Intercept", "Estimate"])*100)`% (intercept) when all predictors were equal to 0: mean participant age (`r printnum(mean(responses$age))` months), mean item lexical frequency (`r printnum(mean(responses$frequency))`, Zipf score), mean participant DoE (`r printnum(mean(responses$doe_center))`%), and regardless of its cognate status (average acros the three levels). An increase in one standard deviation in age (1 *SD* = `r printnum(sd(responses$age))` months) was associated with a `r printnum(coef_comp["age", "Estimate"]*100/4)`% increment in comprehension probability. A one standard deviation increase in lexical frequency (1 *SD* = `r printnum(sd(responses$frequency))` Zipf scores) was associated with a `r printnum(coef_comp["frequency", "Estimate"]*100/4)`% increment in comprehension probability. A 10% increase in relative language exposure made TEs `r printnum(coef_comp["doe_center", "Estimate"]*100/4)`% more likely to be understood. Cognates (identical and non-identical) were `r printnum(coef_comp["cognate1", "Estimate"]*100/4)`% more like to be understood than non-cognates. The 95% credible interval of the `doe_center:cognate1` interaction did not exclude 0. Identical cognates were `r printnum(abs(coef_comp["cognate2", "Estimate"]*100/4))`% less like to be understood than non-identical cognates. The 95% credible interval of the `doe_center:cognate2` interaction excluded 0 (although its lower bound was near zero). Figure 1 summarises the posterior distribution of the fixed effects of the model.

```{r fix_effets_comp, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Point estimate (mean) and credible interval (95\\%) of the posterior distribution of the coefficients of the fixed effects of the best fitting model for comprehension. Points and error bars in indicate the posterior means, and 50% and 95% CrI. The intercept has been transformed using the inverse logit to get the average probability of correct response. The rest of the coefficients has been transformed using the divide-by-four- rule to get the maximum change in probability of correct response, associated with a unit increase in this variable."}

c_comp <- fixef(fits_comp$fit_2) %>% 
  as.data.frame() %>%
  rownames_to_column("term") %>% 
  clean_names() %>% 
  mutate(
    estimate = ifelse(term=="Intercept", inv_logit_scaled(estimate), estimate/4),
    q2_5 = ifelse(term=="Intercept", inv_logit_scaled(q2_5), q2_5/4),
    q97_5 = ifelse(term=="Intercept", inv_logit_scaled(q97_5), q97_5/4),
    est_error = ifelse(term=="Intercept", inv_logit_scaled(est_error), est_error/4)
  ) %>% 
  rename_at(vars(-matches("term")), ~paste0("comp_", .)) %>% 
  select(-one_of(c("rhat", "bulk_ess", "tail_ess")))

c_prod <- fixef(fits_prod$fit_2) %>% 
  as.data.frame() %>%
  rownames_to_column("term") %>% 
  clean_names() %>% 
  mutate(
    estimate = ifelse(term=="Intercept", inv_logit_scaled(estimate), estimate/4),
    q2_5 = ifelse(term=="Intercept", inv_logit_scaled(q2_5), q2_5/4),
    q97_5 = ifelse(term=="Intercept", inv_logit_scaled(q97_5), q97_5/4),
    est_error = ifelse(term=="Intercept", inv_logit_scaled(est_error), est_error/4)
  ) %>% 
  rename_at(vars(-matches("term")), ~paste0("prod_", .)) %>% 
  select(-term, -one_of(c("term", "rhat", "bulk_ess", "tail_ess")))


c <- bind_cols(c_comp, c_prod) 

gt(c) %>% 
  fmt_percent(matches("estimate|error")) %>% 
  fmt_percent(matches("_q")) %>% 
  tab_spanner("Comprehension", matches("comp_")) %>% 
  tab_spanner("Production", matches("prod_")) %>% 
  cols_merge(vars("comp_q2_5", "comp_q97_5"), pattern = "[{1}, {2}]") %>% 
  cols_merge(vars("prod_q2_5", "prod_q97_5"), pattern = "[{1}, {2}]") %>%
  cols_label(
    term = md("**Predictor**"),
    comp_estimate = md("**Mean**"),
    comp_est_error = md("**SE**"),
    comp_q2_5 = md("**95\\% CrI**"),
    comp_q2_5 = md("**95\\% CrI**"),
    prod_q2_5 = md("**95\\% CrI**"),
    prod_estimate = md("**Mean**"),
    prod_est_error = md("**SE**"),
    prod_q2_5 = md("**95\\% CrI**")
  ) %>% 
  tab_footnote(
    footnote = "Transformed using the inverse logit to get the average probability of correct response",
    locations = cells_body(columns = "term", rows = term=="Intercept")
  ) %>% 
  tab_footnote(
    footnote = "Transformed using the divide-by-four- rule to get the maximum change in probability of correct response, associated with a unit increase in this variable.",
    locations = cells_body(columns = "term", rows = term %in% c("age", "frequency", "doe_center", "cognate1", "doe_center:cognate1"))
  ) %>% 
  cols_align(
    align = c("left"),
    columns = 2:4
  ) 
```



## Production

The average probability of participants understanding a TE was `r printnum(inv_logit_scaled(coef_prod["Intercept", "Estimate"])*100)`% (intercept) when all predictors were equal to 0: mean participant age (`r printnum(mean(responses$age))` months), mean item lexical frequency (`r printnum(mean(responses$frequency))`, Zipf score), mean participant DoE (`r printnum(mean(responses$doe_center))`%), and regardless of its cognate status (average acros the three levels). An increase in one standard deviation in age (1 *SD* = `r printnum(sd(responses$age))` months) was associated with a `r printnum(coef_prod["age", "Estimate"]*100/4)`% increment in comprehension probability. A one standard deviation increase in lexical frequency (1 *SD* = `r printnum(sd(responses$frequency))` Zipf scores) was associated with a `r printnum(coef_prod["frequency", "Estimate"]*100/4)`% increment in comprehension probability. A 10% increase in relative language exposure made TEs `r printnum(coef_prod["doe_center", "Estimate"]*100/4)`% more likely to be understood. Cognates (identical and non-identical) were `r printnum(coef_prod["cognate1", "Estimate"]*100/4)`% more like to be understood than non-cognates. The 95% credible interval of the `doe_center:cognate1` interaction did not exclude 0. Identical cognates were `r printnum(abs(coef_comp["cognate2", "Estimate"]*100/4))`% less like to be understood than non-identical cognates. The 95% credible interval of the `doe_center:cognate2` interaction excluded 0 (although its lower bound was near zero). Figure 1 summarises the posterior distribution of the fixed effects of the model.


```{r fix_effets_prod, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Point estimate (mean) and credible interval (95\\%) of the posterior distribution of the coefficients of the fixed effects of the best fitting model for production Points and error bars in indicate the posterior means, and 50% and 95% CrI. The intercept has been transformed using the inverse logit to get the average probability of correct response. The rest of the coefficients has been transformed using the divide-by-four- rule to get the maximum change in probability of correct response, associated with a unit increase in this variable."}

summary(fits_prod$fit_2)$fixed %>% 
  as.data.frame() %>%
  rownames_to_column("term") %>% 
  mutate(
    Estimate = ifelse(term=="Intercept", inv_logit_scaled(Estimate), Estimate/4),
    `l-95% CI` = ifelse(term=="Intercept", inv_logit_scaled(`l-95% CI`), `l-95% CI`/4),
    `u-95% CI` = ifelse(term=="Intercept", inv_logit_scaled(`u-95% CI`), `u-95% CI`/4),
    Est.Error = ifelse(term=="Intercept", inv_logit_scaled(Est.Error), Est.Error/4)
  ) %>% 
  gt() %>% 
  fmt_percent(2:5) %>% 
  fmt_number(6:6, decimals = 2) %>% 
  fmt_number(7:8, decimals = 0) %>% 
  cols_merge(vars("l-95% CI", "u-95% CI"), pattern = "[{1}, {2}]") %>% 
  cols_label(
    term = md("**Predictor**"),
    Estimate = md("**Mean**"),
    Est.Error = md("**SEM**"),
    "l-95% CI" = md("**95\\% CrI**"),
    Rhat = md("**Rhat**"),
    Bulk_ESS = md("**Bulk ESS**"),
    Tail_ESS = md("**Tail ESS**")
  ) %>% 
  tab_footnote(
    footnote = "Transformed using the inverse logit to get the average probability of correct response",
    locations = cells_body(columns = "term", rows = term=="Intercept")
  ) %>% 
  tab_footnote(
    footnote = "Transformed using the divide-by-four- rule to get the maximum change in probability of correct response, associated with a unit increase in this variable.",
    locations = cells_body(columns = "term", rows = term %in% c("age", "frequency", "doe_center", "cognate1", "doe_center:cognate1"))
  ) %>% 
  tab_footnote(
    footnote = "ESS: Effective sample size",
    locations = cells_column_labels(columns = c("Bulk_ESS", "Tail_ESS"))
  ) %>% 
  cols_align(
    align = c("center"),
    columns = 2:4
  ) 
```


```{r emmeans, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Expected marginal mean and 95% credible interval of the posterior probability of comprehension and production for identical cognates, non-identical cognates, and non cognates, plotted separately for three levels of degree of exposue to the language: 0% (no exposure, monolingual), 50% (exposed half of the time, balanced bilingual), and 100% (exposed all of the time, monolingual). Dots represent the mean, and error bars represent the range of values within the true values is contained with 50% (thick line) and 95% (thin line) cprobability, given the data."}

nd <- expand.grid(
  age = mean(responses$age),
  doe_center = c(min(responses$doe_center), mean(responses$doe_center), max(responses$doe_center)),
  cognate = unique(items$cognate),
  frequency_center = 0
)

m_comp <- add_fitted_draws(nd, fits_comp$fit_2, n = 20, re_formula = NA) %>% 
  mutate(
    frequency_center = as.factor(paste0("Frequency = ", frequency_center, " SD")),
    doe_center = factor(doe_center, levels = unique(nd$doe_center), labels = c("DoE: 0%", "Doe: 50%", "DoE: 100%"))
  )

m_prod <- add_fitted_draws(nd, fits_prod$fit_2, n = 20, re_formula = NA) %>% 
  mutate(
    frequency_center = as.factor(paste0("Frequency = ", frequency_center, " SD")),
    doe_center = factor(doe_center, levels = unique(nd$doe_center), labels = c("DoE: 0%", "Doe: 50%", "DoE: 100%"))
  )

m <- bind_rows(m_comp, m_prod, .id = "type") %>% 
  mutate(type = ifelse(type==1, "Comprehension", "Production"))


ggplot(m, aes(cognate, .value, colour = cognate, fill = cognate, shape = doe_center)) +
  facet_grid(~type, scales = "free_y") +
  # geom_line(aes(group = interaction(.draw, cognate), colour = cognate), alpha = 0.25) +
  # stat_lineribbon(size = 0, alpha = 0.5, .width = 0.95) +
  # stat_summary(fun = "mean", geom = "line", size = 1) + 
  geom_hline(yintercept = 0.5, linetype = "dotted") +
  stat_pointinterval(.width = 0.95, position = position_dodge(width = 0.5), size = 0.75, point_size = 2) +
  scale_y_continuous(labels = scales::percent, limits = c(0.1, 0.9), breaks = seq(0, 1, 0.10)) +
  labs(x = "Cognateness", y = "Posterior probability",
       colour = "Cognateness", fill = "Cognateness", shape = "DoE") +
  theme(
    legend.position = "right",
    panel.grid.major.y = element_line(colour = "grey", linetype = "dotted"),
    axis.title.x = element_blank(),
    legend.title = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  ) 




```

# Discussion

# References


\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>

\endgroup


```{r create_r-references, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

r_refs(file = "manuscript.bib")

```

