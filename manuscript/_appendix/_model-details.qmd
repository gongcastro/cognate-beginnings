### Model structure and priors

We used Stan [@carpenter2017stan] as the probabilistic language behind the estimation of our Bayesian models in this study, with `brms` as its R interface [@burkner2017brms]. This language implements the Markov Chain Monte Carlo (MCMC) algorithm using the Hamiltonian Monte Carlo method (HMC) to explore the posterior distribution of the model. Broadly, this algorithm is used to iteratively sample the joint sampling space of the parameters to be estimated in the model, and compute, for each value sampled, its likelihood under some probability distribution previously defined. We run `r english::words(dim(model_fit$fit)[2])` MCMC chains, each `r format(dim(model_fit$fit)[1], big.mark = ",")` iterations long each.

### Considerations on statistical power and sample size

There is little consensus about what approach is adequate for calculating the statistical power of a complex Bayesian model like the one in the present study, for several reasons. A first pitfall, shared with frequentist analysis, is that a closed solution for statistical power calculation is not possible or cannot be computed within reasonable time constraints. This rules out the use of many available pieces of software that are commonly offered for power analysis, as they commonly only consider the case of simpler models like t-tests, ANOVA, Pearson correlation, or regression (with only fixed effects), or trivial derivations of thereof. The more complicated case of multilevel models is usually not covered, not to mention those with a Bayesian approach. 

An alternative way of estimating the statistical power of statistical test is simulation. This consists on simulating multiple datasets in which the hypothesised effect size is present, and fitting multiple instances of the model. The statistical power is derived from the proportion of contrasts that result in the rejection of the null hypothesis across datasets. Although this approach permits the estimation of statistical power in the case of more complex models, it involves costly computations. In the case of Bayesian models, and particularly the one in the present study, such cost can be infeasible. Sampling the posterior of our model took approximately seven days. Running this model, or an equivalent one, across 100 datasets (100 may even be considered too few by many) would take more than a year. 

Following @kruschke2014doing, we considered the precision of our estimates as a proxy to statistical power. In particular, we compared the width of the 95% HDI of the critical regression coefficient (*Exposure* $\times$ *Cognateness*) against some nominal interval width. We decided to use the half the width of the ROPE in the logit scale [-0.025, +0.025], that is, 0.05 as the reference interval width. The width of the fixed regression coefficient of *Exposure* $\times$ *Cognateness* ($\beta$ = `r round(post_draws_list[["b_exposure_std:lv_std"]]$.value/4, 3)`, 95% HDI = [`r round(post_draws_list[["b_exposure_std:lv_std"]]$.lower/4, 3)`, `r round(post_draws_list[["b_exposure_std:lv_std"]]$.upper/4, 3)`]) was `r abs(round(post_draws_list[["b_exposure_std:lv_std"]]$.lower/4 - post_draws_list[["b_exposure_std:lv_std"]]$.upper/4, 3))`, around `r  round(0.05 /  abs(post_draws_list[["b_exposure_std:lv_std"]]$.lower/4 - post_draws_list[["b_exposure_std:lv_std"]]$.upper/4), 3)` times narrower than the reference interval. This indicates that the precision of the posterior 95% HDI of the critical parameter in the model is larger than required.


### Model diagnostics

One way to diagnose the behaviour of HMC is to inspect whether the different MCMC chains (if more than one) have converged to a similar region of the posterior. The Gelman-Rubin diagnostic [$\hat{R}$ or R-hat @gelman1992inference] provides a measure of chain convergence by comparing the variance within each chain *versus* the variance between each chain. Both are expected to be identical when chains have perfectly converged, so that $\hat{R} = 1$. Values lower than 1.01 are recommended, while values higher than 1.05 indicate that chains might have trouble converging and therefore the estimated parameters must be taken with caution. @fig-rhats-neffs (A) shows the distribution of $\hat{R}$ values for the coefficients of the fixed effect of our models, which we used for statistical inference. Most values are lower than 1.01, and never higher than 1.05, which provides evidence of successful MCMC convergence.

Another diagnostic of good MCMC converge is the ratio of effective sample size to total sample size ($N_{eff}/N$), which indicates the proportion of samples in the chain that resulted from a non-divergent transition. Values closer to 1 are ideal, as they indicate that all posterior samples from the MCMC were used to estimate the posterior distribution of the parameter. Values larger than 0.1 are recommended. @fig-rhats-neffs (B) shows the distribution of the effective sample sizes of the coefficients of the fixed effects in our models. Most values are larger than 0.1, although model 0 ($\mathcal{M}_0$) accumulates most effective sample sizes close to 0.1.

```{r fig-rhats-neffs}
#| label: fig-rhats-neffs
#| fig-cap: "MCMC convergence diagnostic of all parameters in the model. Each dot represents the score of one parameter. (A) Distribution of the Gelman-Rubin (R-hat) scores. (B) Distribution of the ratio of effective sample size."
#| echo: false
#| message: false
#| warning: false
#| fig-height: 3
#| fig-width: 6
#| out-width: 100%
model_convergence |> 
    ggplot(aes(.rhat)) +
    geom_dots(colour = "black") +
    geom_vline(xintercept = 1.01, linetype = "dashed") +
    labs(
        x = "R-hat",
        y = "MCMC samples"
    ) +
    model_convergence |> 
    ggplot(aes(.neff)) +
    geom_dots(colour = "black", na.rm = TRUE) +
    geom_vline(xintercept = 1.01, linetype = "dashed") +
    labs(
        x = "Effective sample size ratio",
        y = "MCMC samples"
    ) +    
    plot_layout(ncol = 1) &
    plot_annotation(tag_levels = "A") &
    theme(
        legend.position = "none",
        panel.grid = element_blank()
    )

save_image("_assets/img/Figure_1_SuppInfo", 
           dpi = 1e3L, width = 6, height = 3)
```

Another way of assessing the behaviour of the HMC algorithm is to visualise the joint posterior distribution for pairs of parameters using bi-variate scatter plots. In @fig-model-pairs we show the pair-wise distribution of posterior samples. Broadly, posterior samples of two parameters should not be correlated. This is the case for all pairs of parameters but for the two intercepts. This is expected behaviour, given that these two parameters correspond to the thresholds between categories in the ordinal regression model, and the distance between both thresholds is fixed in the particular parametrisation of the model. 

```{r}
#| label: fig-model-pairs
#| fig-cap: "Marginal distribution and bi-variate scatterplot of posterior samples for the fixed regression coefficients in Model 3."
#| fig-height: 8
#| fig-width: 10
#| out-width: 100%
#| out-height: 100%
#| echo: false
#| warning: false
#| message: false
titles <- c("Comprehension",
            "Production",
            "Age",
            "Length",
            "Exposure",
            "Cognateness",
            "Age × Exposure",
            "Age × Cognateness",
            "Exposure × Cognateness",
            "Age × Exposure × Cognateness")

posterior <- as_draws_df(model_fit) |>
    select(`b_Intercept[1]`:`b_age_std:exposure_std:lv_std`)

my_diag <- function(data, mapping, ...) {
    ggplot(data = data, mapping = mapping) +
        stat_slab(
            colour = "white",
            fill = "grey15"
        )
}

my_upper <- function(data, mapping, ...) {
    ggplot(data = data, mapping = mapping) +
        stat_density_2d(aes(fill = ..density..),
                        geom = "raster",
                        contour = FALSE
        ) +
        scale_fill_distiller(
            palette = "Greys",
            direction = 1
        )
}

posterior |> 
    GGally::ggpairs(
        upper = list(continuous = my_upper),
        diag = list(continuous = my_diag),
        lower = NULL,
        columnLabels = c("Intercept\n(Comprehension)",
                         "Intercept\n(Production)",
                         "Age", "Length", "Exposure",
                         "Levenshtein", "Age × Exposure",
                         "Age × Levenshtein", "Exposure × Leveshtein",
                         "Age × Exposure ×\nLevenshtein"
        ),
        axisLabels = "none"
    ) +
    theme(strip.text = element_text(size = 5),
          panel.grid = element_blank(),
          panel.border = element_blank()
    )

save_image("_assets/img/Figure_2_SuppInfo",
           dpi = 1e3L, width = 10, height = 8)

```


Finally, we also assessed the predictive performance of the model by doing posterior-predictive checks (PPCs). this involves simulating new datasets from the model and the posterior distribution of its parameters, and checking that, overall, the distribution of the response variable across the simulated datasets is equivalent to the one in the observed dataset. @fig-ppc shows the PPCs of the model.

```{r fig-ppc}
#| label: fig-ppc
#| fig-cap: "Model posterior predictive checks (PPC). Bars indicate the observed proportion of responses to each category (No, Understands, and Understands and Says). Blue dots and error bars represent the mean proportion of responses simulated from the posterior for each category, and its 95\\% interval."
#| eval: true
#| echo: false
#| warning: false
#| message: false
#| fig-width: 6
#| fig-height: 3
#| out-width: 80%
#| out-height: 80%
y_int <- as.integer(as.integer(model_fit$data$response))
bayesplot::color_scheme_set(c(rep("grey60", 2), rep("black", 4)))

bayesplot::ppc_bars(
    y_int,
    model_ppcs,
    prob = 0.95,
    linewidth = 1,
    size = 1.25,
    fatten = 2,
    freq = FALSE
) +
    scale_x_continuous(
        breaks = 1:3,
        labels = c(
            "No",
            "Understands",
            "Understands and Says"
        )
    ) +
    theme(
        legend.position = "none",
        axis.text = element_text(size = 11),
        panel.grid = element_blank()
    )

save_image("_assets/img/Figure_3_SuppInfo",
           dpi = 1e3L, width = 6, height = 3)
```
